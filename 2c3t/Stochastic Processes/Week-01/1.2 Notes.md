Frequently we may know that some event $B$ in $\Omega$ has in fact occurred; or

we may wish to investigate the consequences if it were supposed that it

had. The sample space $\Omega$ is effectively reduced to $B$, and considerations of

proportion lead us to define the concept of conditional probability thus:

$( 1) \textbf{ Conditional probability. If P}( B) > 0$, then the conditional probability

that $A$ occurs given that $B$ occurs is denoted by P$(A\mid B)$, and defined by

$$\mathrm P(A\mid B)=\mathrm P(A\cap B)/\mathrm P(B).$$

(2)

More generally, whether P(B)>0 or not, we agree that

$$\mathrm P(A\cap B)=\mathrm P(A\mid B)\mathrm P(B).$$

(3)

This is sometimes called the basic __multiplication rule__. $\triangle$


It is most important to remember that $\mathsf{P}(A\mid B)$ is a probability function as

we defined above, so that if we define, for each $A$ in $\mathcal{F}$,the function

$$Q(A)=\mathrm{P}(A\mid B),$$

this satisfes all the basic properties in (1.1.6), and consequent results. For

example (1.1.7) becomes (4)

$$\mathrm P(A^c\mid B)=1-\mathrm P(A\mid B)$$

and so on. Furthermore, the definition of conditional probability, together 

with the basic rules for probability, supply several further important and useful

identities. For example,


(5) __Partition Rule__. In its simplest form this says

$$\mathrm P(A)=\mathrm P(A\mid B)\mathrm P(B)+\mathrm P(A\mid B^c)\mathrm P(B^c).$$

More generally, if $B_j\cap B_k=\emptyset$ for $j\neq k$,and $A\subseteq\bigcup_rB_r$,then

$$
P(A) = \sum_{r} P(A \mid B_r) P(B_r).
$$

Both identities follow immediately from the definition of conditional probability

and (1.1.11).


(6) __Multiplication Rule.__ The elementary form P(A âˆ© B) = P(A | B)P(B) in (3) is

easily extended to the more general form

$$P\left(\bigcap_{r=1}^{n+1} A_r\right) = P(A_1)P(A_2|A_1)P(A_3|A_1\cap A_2)\cdots P\left(A_{n+1}\left|\bigcap_{r=1}^n A_r\right.\right).$$

This is a simple exercise for you.


Finally, if we combine the extended Partition Rule (5), with definition (2), we

obtain the celebrated

(7) __Bayes' Rule__. If $A_j \cap A_k = \emptyset$ for $j \neq k$, and $B \subseteq \bigcup_{r=1}^{n} A_r$, and P(B) > 0, then

$$P(A_r | B) = \frac{P(B | A_r)P(A_r)}{\sum_{r=1}^{n} P(B | A_r)P(A_r)}.$$

A popular application of this result is to the interpretation of test results; see

the exercises.


The idea of conditioning leads us to another key concept. It may happen that

P(A | B) = P(A), and this is sufficiently important to deserve a definition:

(8) __Independence__

(a) Events A and B are independent if

A family $(A_i, 1 \leq i \leq n)$ is independent if $$P(A_{i_1} \cap A_{i_2} \cap \cdots \cap A_{ir}) = P(A_{i_1}) \cdot P(A_{i_2}) \cdot \cdots \cdot P(A_{ir})$$ for any selection $1 \leq i_1 < i_2 < \cdots < i_r \leq n$. A family $(A_i, 1 \leq i \leq n)$ is 

pairwise independent if $$P(A_i \cap A_j) = P(A_i)P(A_j), i \neq j$$. This is implied by independence, but not vice versa. Events $A$ and $B$ are 

conditionally independent given $C$ if  $$P(A \cap B | C) = P(A | B)(P(A | C))$$
This does not imply the independence of $A$ and $B$, nor is it implied by the 

independence of $A$ and $B$ (except in trivial special cases).$\triangle$



